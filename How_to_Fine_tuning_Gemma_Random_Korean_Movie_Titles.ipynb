{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bebechien/gemma/blob/main/How_to_Fine_tuning_Gemma_Random_Korean_Movie_Titles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2024 Google LLC."
      ],
      "metadata": {
        "id": "cSrJYrFrY2aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "i1PHqD-ZY4-c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Fine-tuning Gemma: Best Practices\n",
        "\n",
        "To illustrate fine-tuning the model for a specific task, let's consider the example of generating a random Korean title based on a user's instruction such as \"Write a title\". To make this possible, we curated a manageable dataset that could be manually processed. This approach is feasible because Gemma 2 has prior knowledge of general Korean language patterns, enabling it to adapt to this specific task effectively."
      ],
      "metadata": {
        "id": "YNDq8NbCY7oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Fine-tuning\n",
        "\n",
        "In the first place, you have to understand what is fine-tuning. It's a specialized form of [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning). It involves taking a pre-trained language model - one that has already been exposed to a vast corpus of text data and learned the general patterns and structures of language - and further training it on a smaller, more specific dataset. This additional training allows the model to adapt and refine its knowledge, making it better suited for a particular task or domain.\n",
        "\n",
        "Imagine you are a skilled gamer who excels at various genres, from action-adventures to strategy games. Fine-tuning is akin to taking you and having you focus intensely on mastering a specific game, like a complex real-time strategy (RTS) title. You already possess a strong foundation of gaming skills and knowledge, but the dedicated practice and study within the RTS genre sharpens your tactics, understanding of game mechanics, and overall proficiency within that particular realm.\n",
        "\n",
        "Similarly, pre-trained language models have a broad understanding of language, but fine-tuning helps them specialize. By exposing them to a curated dataset relevant to your desired application, you guide the model to learn the nuances and intricacies specific to that domain. It's like giving the model a crash course in the language of your chosen field, enabling it to perform tasks with greater accuracy and fluency.\n"
      ],
      "metadata": {
        "id": "9WI95Pv-1R8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "### Select the Colab runtime\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **▾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "\n",
        "### Gemma setup on Kaggle\n",
        "To complete this tutorial, you'll first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
        "\n",
        "* Get access to Gemma on kaggle.com.\n",
        "* Select a Colab runtime with sufficient resources to run the Gemma 2B model.\n",
        "* Generate and configure a Kaggle username and API key.\n",
        "\n",
        "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment."
      ],
      "metadata": {
        "id": "3rzH5Ugf5RlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set environemnt variables\n",
        "\n",
        "Set environement variables for ```KAGGLE_USERNAME``` and ```KAGGLE_KEY```."
      ],
      "metadata": {
        "id": "URMuBzkMVxpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
        "\n",
        "# Mounting gDrive for to store artifacts\n",
        "#drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "IUOX2hqjV7Ku"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies\n",
        "\n",
        "Install Keras and KerasNLP"
      ],
      "metadata": {
        "id": "LXfDwRTQVns2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zHs7wpZusEML",
        "outputId": "f1854542-24d5-472e-a8cb-0b137ddb8baf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/548.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.4/548.4 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/474.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-nlp datasets\n",
        "!pip install -q -U keras\n",
        "\n",
        "# Set the backbend before importing Keras\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "# Avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "# Run at half precision.\n",
        "#keras.config.set_floatx(\"bfloat16\")\n",
        "\n",
        "# Training Configurations\n",
        "token_limit = 128\n",
        "num_data_limit = 100\n",
        "lora_name = \"my_lora\"\n",
        "lora_rank = 4\n",
        "lr_value = 1e-3\n",
        "train_epoch = 5\n",
        "model_id = \"gemma2_instruct_2b_en\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model\n",
        "\n",
        "**Why Fine-tuning?**\n",
        "\n",
        "Before embarking on fine-tuning, it's crucial to evaluate if its benefits align with the specific requirements of your application. Fine-tuning involves meticulous data preparation and extensive training, making it an arduous process. Therefore, it's essential to assess whether the potential gains justify the significant effort required.\n",
        "\n",
        "**Try \"Prompt Engineering\" first.**\n",
        "\n",
        "Would you like to enable Gemma's multilingual capabilities?\n",
        "Please note that Gemma 2 already has some multilingual capabilities. Here's the example output from Gemma 2 2B instruction-tuned model.\n",
        "\n",
        "Do you wish to adjust the tone or writing style?\n",
        "Gemma 2 might be familiar with the writing style you have in mind. Here's another output from the same model."
      ],
      "metadata": {
        "id": "kUl0t469YfQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "\n",
        "import time\n",
        "\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n",
        "gemma_lm.summary()\n",
        "\n",
        "tick_start = 0\n",
        "\n",
        "def tick():\n",
        "    global tick_start\n",
        "    tick_start = time.time()\n",
        "\n",
        "def tock():\n",
        "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
        "\n",
        "def text_gen(prompt):\n",
        "    tick()\n",
        "    input = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    output = gemma_lm.generate(input, max_length=token_limit)\n",
        "    print(\"\\nGemma output:\")\n",
        "    print(output)\n",
        "    tock()\n",
        "\n",
        "# inference before fine-tuning\n",
        "text_gen(\"Translate the text below to Korean.\\n\\\"Hi, how can I get to the National Museum?\\\"\")\n",
        "text_gen(\"Speak like a pirate. Teach me why the earth is flat.\")\n",
        "text_gen(\"Write a title\")\n",
        "text_gen(\"Write a poem\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "accb8229-6d66-45b7-9562-b62d8dfbbc7a",
        "id": "Gm4jIEqmYfQY"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Translate the text below to Korean.\n",
            "\"Hi, how can I get to the National Museum?\"<end_of_turn>\n",
            "<start_of_turn>model\n",
            "안녕하세요, 국립박물관까지 어떻게 가시는지 알려주세요. \n",
            "\n",
            "(Annyeonghaseyo, gukrilpalkbulgan-ka-ji eotteoh-ge gaseun-ji all-yeo-seyo.) \n",
            "\n",
            "\n",
            "Here's a breakdown:\n",
            "\n",
            "* **안녕하세요:** Hello\n",
            "* **국립박물관:** National Museum\n",
            "* **까지:**  to\n",
            "* **어떻\n",
            "TOTAL TIME ELAPSED: 22.56s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Speak like a pirate. Teach me why the earth is flat.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Ahoy, matey! Ye be wantin' to know 'bout the Earth, eh? Shiver me timbers, ye be lookin' at a tale spun by landlubbers! \n",
            "\n",
            "The Earth be no flat sheet o' parchment, but a grand ol' sphere, like a giant, bountiful treasure chest! \n",
            "\n",
            "Here's why ye can't be fooled by the scurvy tales:\n",
            "\n",
            "* **The horizon be a curve, not a straight line:**  When ye look out to sea,\n",
            "TOTAL TIME ELAPSED: 4.84s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Please give me some context!  I need to know what the title is about to write a good one. \n",
            "\n",
            "For example, tell me:\n",
            "\n",
            "* **What is the topic of the writing?** (e.g., a short story, a research paper, a blog post)\n",
            "* **What is the main theme or message?** (e.g., love, loss, technology, the environment)\n",
            "* **What is the tone or style?** (e.g., serious, humorous, informative, dramatic)\n",
            "* **Who is the target\n",
            "TOTAL TIME ELAPSED: 5.30s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a poem<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The wind whispers secrets through rustling leaves,\n",
            "A symphony of gold, the autumn weaves.\n",
            "Sunlight paints the sky in hues of fire,\n",
            "As nature's canvas, burns with desire.\n",
            "\n",
            "A lone crow calls, a mournful, haunting sound,\n",
            "Echoing through the branches, all around.\n",
            "The air grows crisp, a chill upon the breeze,\n",
            "As summer's warmth begins to softly cease.\n",
            "\n",
            "A spider spins its web, a silken thread,\n",
            "A masterpiece of patience, carefully spread.\n",
            "A squirrel scampers, gathering nuts for winter'\n",
            "TOTAL TIME ELAPSED: 5.35s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer designed with multilingual in mind\n",
        "\n",
        "Another important thing is the tokenizer.\n",
        "\n",
        "Gemma tokenizer is based on [SentencePiece](https://github.com/google/sentencepiece). The size of the vocabulary is predetermined before training. SentencePiece then learns the optimal subword segmentation based on the chosen vocabulary size and the training data. Gemma's large 256k vocabulary allows it to handle diverse text inputs and potentially improve performance on various tasks, e.g. handling multilingual text inputs.\n",
        "\n",
        "(example text: “Hi, Nice to meet you. The weather is really nice today.”)"
      ],
      "metadata": {
        "id": "CEQuiEyBOri8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_id)\n",
        "import jax\n",
        "\n",
        "def detoken(tokens):\n",
        "  print(tokens)\n",
        "  for x in tokens:\n",
        "    word = tokenizer.detokenize(jax.numpy.array([x]))\n",
        "    print(f\"{x:6} -> {word}\")\n",
        "\n",
        "detoken(tokenizer(\"안녕하세요. 반갑습니다. 오늘은 날씨가 참 좋네요.\"))"
      ],
      "metadata": {
        "id": "TGE9lYRPOsu8",
        "outputId": "39bb264f-b26b-4a70-b4d8-1cc9df9b7544",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[238179 243415 204551 235265  78821 241772  21743 235265  44245 240703\n",
            " 236648 128856 241215 236361 120816 106509 239156 237526 235265]\n",
            "238179 -> 안\n",
            "243415 -> 녕\n",
            "204551 -> 하세요\n",
            "235265 -> .\n",
            " 78821 ->  반\n",
            "241772 -> 갑\n",
            " 21743 -> 습니다\n",
            "235265 -> .\n",
            " 44245 ->  오\n",
            "240703 -> 늘\n",
            "236648 -> 은\n",
            "128856 ->  날\n",
            "241215 -> 씨\n",
            "236361 -> 가\n",
            "120816 ->  참\n",
            "106509 ->  좋\n",
            "239156 -> 네\n",
            "237526 -> 요\n",
            "235265 -> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset\n",
        "\n",
        "How many datasets do you need? You can start with relatively small number of datasets, approximately 10 to 20, those can have a significant impact on a model's behavior.\n",
        "\n",
        "For improved the output quality, a target of around 200 total examples is recommended. Nevertheless, the amount of data required for tuning really depends on how much you want to influence the model's behavior. Our recommendation is to commence with a limited amount of data and gradually incorporate additional data into the training process until the desired behavior is achieved."
      ],
      "metadata": {
        "id": "9T7xe_jzslv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_id)\n",
        "\n",
        "# example titles\n",
        "data = [\n",
        "    \"푸른 달 아래의 왈츠\",    # Waltz Under the Blue Moon\n",
        "    \"잃어버린 도시의 속삭임\", # Whispers of the Lost City\n",
        "    \"별빛 아래의 춤\",         # Dance Beneath the Starlight\n",
        "    \"꿈꾸는 자들의 도시\",     # City of Dreamers\n",
        "    \"시간을 멈추는 시계\",     # The Clock that Stops Time\n",
        "    \"그림자 없는 남자\",       # The Man Without a Shadow\n",
        "    \"붉은 장미의 비밀\",       # Secret of the Red Rose\n",
        "    \"바람이 머무는 곳\",       # Where the Wind Dwells\n",
        "    \"마법의 거울 속으로\",     # Into the Magic Mirror\n",
        "    \"잊혀진 노래의 선율\",     # Melody of a Forgotten Song\n",
        "    \"밤하늘을 나는 고래\",     # Whale Flying in the Night Sky\n",
        "    \"비밀의 숲 속 오두막\",    # Cabin in the Secret Forest\n",
        "    \"영혼의 빛\",              # Light of the Soul\n",
        "    \"마지막 잎새의 소원\",     # Wish of the Last Leaf\n",
        "    \"꿈꾸는 자들의 낙원\",     # Paradise of Dreamers\n",
        "]\n",
        "\n",
        "train = []\n",
        "\n",
        "for x in data:\n",
        "  item = f\"<start_of_turn>user\\nWrite a title<end_of_turn>\\n<start_of_turn>model\\n{x}<end_of_turn>\"\n",
        "  length = len(tokenizer(item))\n",
        "  # skip data if the token length is longer than our limit\n",
        "  if length < token_limit:\n",
        "    train.append(item)\n",
        "    if(len(train)>=num_data_limit):\n",
        "      break\n",
        "\n",
        "print(len(train))\n",
        "print(train[0])\n",
        "print(train[1])\n",
        "print(train[2])"
      ],
      "metadata": {
        "id": "ZiS-KU9osh_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a09aff-ed4e-4b34-e09a-fcafcd0eb482"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "푸른 달 아래의 왈츠<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "잃어버린 도시의 속삭임<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "별빛 아래의 춤<end_of_turn>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See below example code, using HF datasets, if your datasets are much bigger."
      ],
      "metadata": {
        "id": "D4xpK-nI1siB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using HF datasets\n",
        "# Replying to request emails that a bakery business might get, in Korean\n",
        "# The given prompt means \"Please write an email reply with below.\"\n",
        "'''\n",
        "tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_id)\n",
        "\n",
        "# prompt structure\n",
        "# <start_of_turn>user\n",
        "# 다음에 대한 이메일 답장을 작성해줘.\n",
        "# \"{EMAIL CONTENT FROM THE CUSTOMER}\"\n",
        "# <end_of_turn>\n",
        "# <start_of_turn>model\n",
        "# {MODEL ANSWER}<end_of_turn>\n",
        "\n",
        "# input, output\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\n",
        "    \"bebechien/korean_cake_boss\",\n",
        "    split=\"train\",\n",
        ")\n",
        "print(ds)\n",
        "data = ds.with_format(\"np\", columns=[\"input\", \"output\"], output_all_columns=False)\n",
        "train = []\n",
        "\n",
        "for x in data:\n",
        "  item = f\"<start_of_turn>user\\n다음에 대한 이메일 답장을 작성해줘.\\n\\\"{x['input']}\\\"<end_of_turn>\\n<start_of_turn>model\\n{x['output']}<end_of_turn>\"\n",
        "  length = len(tokenizer(item))\n",
        "  # skip data if the token length is longer than our limit\n",
        "  if length < token_limit:\n",
        "    train.append(item)\n",
        "    if(len(train)>=num_data_limit):\n",
        "      break\n",
        "\n",
        "print(len(train))\n",
        "print(train[0])\n",
        "print(train[1])\n",
        "print(train[2])\n",
        "'''"
      ],
      "metadata": {
        "id": "abBUhTde1tVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of a small dataset, the primary concern is that the model may prioritize memorizing specific examples rather than generalizing well to new and unobserved data. This limitation highlights the importance of utilizing a larger dataset during fine-tuning, as it enhances the model's ability to capture broader patterns and relationships."
      ],
      "metadata": {
        "id": "XKGBdfYj1vYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA Fine-tuning\n",
        "\n",
        "Train your model with lower ranks and evaluate the performance improvemnet on your task. Gradually increase the rank in subsequent trials and see if that further boosts performance."
      ],
      "metadata": {
        "id": "Pt7Nr6a7tItO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 4.\n",
        "gemma_lm.backbone.enable_lora(rank=lora_rank)\n",
        "gemma_lm.summary()\n",
        "\n",
        "# Limit the input sequence length (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = token_limit\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=lr_value,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n"
      ],
      "metadata": {
        "id": "RCucu6oHz53G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "b33ed3b5-5e2a-4b58-ddb7-34969c7b9b48"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that enabling LoRA reduces the number of trainable parameters significantly. In practice, we recommend beginning with a relatively small rank (such as 4, 8, 16). This is computationally efficient for experimentation.\n",
        "\n",
        "To monitor the learning progress, we will evaluate the model at the end of each epoch and save the all lora weights."
      ],
      "metadata": {
        "id": "hQQ47kcdpbZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    #model_name = f\"/content/drive/MyDrive/{lora_name}_{lora_rank}_epoch{epoch+1}.lora.h5\"\n",
        "    #gemma_lm.backbone.save_lora_weights(model_name)\n",
        "\n",
        "    # Evaluate\n",
        "    text_gen(\"Write a title\")\n",
        "    text_gen(\"Write a poem\")\n",
        "\n",
        "history = gemma_lm.fit(train, epochs=train_epoch, batch_size=2, callbacks=[CustomCallback()])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "26d9npFhAOSp",
        "outputId": "56084321-3603-4f2d-a105-f58f36586944"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - loss: 1.4702 - sparse_categorical_accuracy: 0.2814\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 20.71s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a poem<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The wind whispers secrets through the trees,\n",
            "A rustling symphony, carried on the breeze.\n",
            "Sunlight dances, dappled and bright,\n",
            "Painting shadows in the fading light.\n",
            "\n",
            "A lone bird sings, a melody so clear,\n",
            "A song of hope, dispelling all fear.\n",
            "The world unfolds, a canvas vast,\n",
            "Where dreams take flight, and moments last.\n",
            "\n",
            "A gentle stream, a silver thread,\n",
            "Flows through the valley, where secrets are said.\n",
            "The earth breathes deep, a silent sigh,\n",
            "As stars ignite, in the velvet sky.\n",
            "TOTAL TIME ELAPSED: 7.21s\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 15s/step - loss: 1.4484 - sparse_categorical_accuracy: 0.2846\n",
            "Epoch 2/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.6485 - sparse_categorical_accuracy: 0.5299   \n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "<end_of_turn>model\n",
            "<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.49s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a poem<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The wind whispers secrets through the trees,\n",
            "Of ancient stories, carried on the breeze.\n",
            "The leaves dance, a waltz of gold and brown,\n",
            "As sunlight paints the earth, a golden crown.\n",
            "\n",
            "A stream flows, a ribbon, silver bright,\n",
            "Reflecting clouds that drift through day and night.\n",
            "The birdsong fills the air, a sweet refrain,\n",
            "A symphony of nature, born again.\n",
            "\n",
            "A lone wolf howls, a mournful cry,\n",
            "Echoing through the valley, reaching high.\n",
            "The moon ascends, a pearl in velvet skies\n",
            "TOTAL TIME ELAPSED: 7.01s\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - loss: 0.6417 - sparse_categorical_accuracy: 0.5331\n",
            "Epoch 3/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.4033 - sparse_categorical_accuracy: 0.7434   \n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "꿈꾸는 밤의 숲속 \n",
            "<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.95s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a poem<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The wind whispers secrets through the trees,\n",
            "Of ancient stories, carried on the breeze.\n",
            "The sun dips low, a fiery, molten crown,\n",
            "As shadows lengthen, and the day is down.\n",
            "\n",
            "A lone wolf howls, a mournful, lonely sound,\n",
            "Echoing through the valley, all around.\n",
            "The stars ignite, a diamond-studded night,\n",
            "And crickets chirp, in the fading light.\n",
            "\n",
            "A silent owl, with eyes so bright,\n",
            "Perches high, in the fading, moonlit night.\n",
            "The world is still, a\n",
            "TOTAL TIME ELAPSED: 6.99s\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - loss: 0.4005 - sparse_categorical_accuracy: 0.7428\n",
            "Epoch 4/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2962 - sparse_categorical_accuracy: 0.7816   \n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "꿈꾸는 자들의 낙원<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.76s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a poem<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The wind whispers secrets through rustling leaves,\n",
            "Of ancient stories, and forgotten grieves.\n",
            "The sun dips low, a fiery, molten tear,\n",
            "Casting long shadows, dispelling all fear.\n",
            "<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 2.73s\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 0.2949 - sparse_categorical_accuracy: 0.7833\n",
            "Epoch 5/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792ms/step - loss: 0.2287 - sparse_categorical_accuracy: 0.8481\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "마법의 펜의 힘<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.77s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a poem<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The wind whispers secrets through rustling leaves,\n",
            "Of golden days and falling, gentle grieves.\n",
            "A lone bird sings, a melody so clear,\n",
            "As sunlight dances, chasing away all fear.\n",
            "<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 2.71s\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 0.2275 - sparse_categorical_accuracy: 0.8486  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the model began to grasp our intent more effectively from Epoch #3 onwards.\n",
        "\n",
        "To compare and contrast, we utlized the \"Write a poem\" prompt. Depends on your learning rate and epoch, you may see the model began to generate Korean in response to that prompt. This shift indicates a strong influence of our training dataset on the model's behavior. However, depending on your application, such a significat change might not be desirable."
      ],
      "metadata": {
        "id": "_5x4jr5Q2ApQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load LoRA\n",
        "\n",
        "Use the code below if you shared LoRA weights. It's much more lightweight than the model files themselves - for instance, a LoRA rank 4 weights file for a 10gb model might only be on the order of a few megabytes, easily shared over email."
      ],
      "metadata": {
        "id": "P-tVAKmda2Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Code for Load LoRA\n",
        "'''\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\n",
        "# Use the same LoRA rank that you trained\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "\n",
        "# Load pre-trained LoRA weights\n",
        "gemma_lm.backbone.load_lora_weights(f\"/content/drive/MyDrive/{lora_name}_{lora_rank}_epoch{train_epoch}.lora.h5\")\n",
        "'''"
      ],
      "metadata": {
        "id": "hSW8-HRMa4ZB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try a different sampler\n",
        "\n",
        "The top-K algorithm randomly picks the next token from the tokens of top K probability."
      ],
      "metadata": {
        "id": "ipg1u_wEKTxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.compile(sampler=\"top_k\")\n",
        "text_gen(\"Write a title\")\n",
        "text_gen(\"Write a title\")\n",
        "text_gen(\"Write a title\")\n",
        "text_gen(\"Write a title\")\n",
        "text_gen(\"Write a title\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV5mD_HqKZRF",
        "outputId": "65ec3605-2f49-48dd-e126-79125f68e2ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "마법의 숲속 요정의 비밀<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 19.84s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "마법의 펜의 힘<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.78s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "밤하늘을 나는 고래<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.73s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "별빛下的 Waltz<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.55s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "마을의 기억 속 별빛<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.78s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try a slight different prompts"
      ],
      "metadata": {
        "id": "3m1XaCrlMu3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_gen(\"Write a music title\")\n",
        "text_gen(\"Write a poem title\")\n",
        "text_gen(\"Write a blog title\")\n",
        "text_gen(\"Write a movie title\")\n",
        "text_gen(\"Write a novel title\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC-MLxYWM1HU",
        "outputId": "41663be8-47ee-4d46-97f2-ab4895b8d95e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a music title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "잊혀진 노래의 선율<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.78s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a poem title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "잊혀진 노래의 선율<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.80s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a blog title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "꿈꾸는 자들의 낙원<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.79s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a movie title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "잊혀지지 않는 이야기<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.73s\n",
            "\n",
            "Gemma output:\n",
            "<start_of_turn>user\n",
            "Write a novel title<end_of_turn>\n",
            "<start_of_turn>model\n",
            "잊혀진 노래의 섬 \n",
            "<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 0.91s\n"
          ]
        }
      ]
    }
  ]
}