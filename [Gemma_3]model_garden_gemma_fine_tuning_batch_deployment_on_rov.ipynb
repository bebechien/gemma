{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Get started with Gemma on Ray on Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/bebechien/gemma/blob/main/[Gemma_3]model_garden_gemma_fine_tuning_batch_deployment_on_rov.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fbebechien%2Fgemma%2Fmain%2F%5BGemma_3%5Dmodel_garden_gemma_fine_tuning_batch_deployment_on_rov.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/bebechien/gemma/blob/main/[Gemma_3]model_garden_gemma_fine_tuning_batch_deployment_on_rov.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/bebechien/gemma/blob/main/[Gemma_3]model_garden_gemma_fine_tuning_batch_deployment_on_rov.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use Ray on Vertex AI for fine-tuning and serving Gemma on Vertex AI.\n",
        "\n",
        "Learn more about [Ray on Vertex AI](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you'll learn how to distribute Gemma Supervised tuning on Ray on Vertex AI. Furthermore, you'll learn how to deploy the trained model seamlessly for offline predictions using Ray Data on Ray on Vertex AI.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Ray on Vertex AI\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Create a Ray cluster on Vertex AI\n",
        "- Tune Gemma with Ray Train on Ray on Vertex AI\n",
        "- Serving Gemma with Ray Data for offline predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The [Extreme Summarization (XSum) dataset](https://huggingface.co/datasets/EdinburghNLP/xsum) is a dataset about abstractive single-document summarization systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUKGInbrh0Cw"
      },
      "source": [
        "<b>Note</b>: This tutorial uses the Ray Jobs API via public Ray Dashboard. The Ray dashboard address is accessible from outside the VPC, including the public internet. To learn more about  private versus public connectivity, see the [Private and public connectivity](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster#private_and_public_connectivity) section in the [Create a Ray cluster on Vertex AI](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster) documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,artifactregistry.googleapis.com,cloudbuild.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    USER = \"--user\"\n",
        "else:\n",
        "    USER = \"\"\n",
        "\n",
        "! pip3 install {USER} google-cloud-aiplatform[ray]==1.87.0 -q --no-warn-conflicts\n",
        "! pip3 install {USER} google-cloud-aiplatform[tensorboard]==1.87.0 -q --no-warn-conflicts\n",
        "! pip3 install {USER} datasets==3.5.0 evaluate==0.4.3 peft==0.15.1 -q --no-warn-conflicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CylYbUxrx3W-"
      },
      "source": [
        "### Set Google Cloud project information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Project ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3546c0b3-00f0-44ca-9166-0d7d9d8acdc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"ray-test-456802\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPrDj6HE9_EU"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "You create a timestamp to make resources you create unique in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6Le1schAziq"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "#### Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = f\"ray-test-{PROJECT_ID}-unique\"  # @param\n",
        "\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6319403-c984-4812-c796-5c78cbdc4e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://ray-test-ray-test-456802-unique/...\n"
          ]
        }
      ],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "Set service account and grant the service account access to Vertex AI TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_C_BMVpzhug"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"ray-test@ray-test-456802.iam.gserviceaccount.com\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR2GDIkzp4af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d735e1-eff4-4aa2-faed-ea51173bce76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated IAM policy for project [ray-test-456802].\n",
            "bindings:\n",
            "- members:\n",
            "  - serviceAccount:ray-test@ray-test-456802.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.user\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
            "  role: roles/artifactregistry.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:626103750140@cloudbuild.gserviceaccount.com\n",
            "  role: roles/cloudbuild.builds.builder\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
            "  role: roles/cloudbuild.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@containerregistry.iam.gserviceaccount.com\n",
            "  role: roles/containerregistry.ServiceAgent\n",
            "- members:\n",
            "  - user:bebechien@google.com\n",
            "  role: roles/owner\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-pubsub.iam.gserviceaccount.com\n",
            "  role: roles/pubsub.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:626103750140-compute@developer.gserviceaccount.com\n",
            "  role: roles/storage.admin\n",
            "- members:\n",
            "  - serviceAccount:ray-test@ray-test-456802.iam.gserviceaccount.com\n",
            "  role: roles/storage.objectViewer\n",
            "etag: BwYytHnxVcY=\n",
            "version: 1\n",
            "Updated IAM policy for project [ray-test-456802].\n",
            "bindings:\n",
            "- members:\n",
            "  - serviceAccount:ray-test@ray-test-456802.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.user\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
            "  role: roles/artifactregistry.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:626103750140@cloudbuild.gserviceaccount.com\n",
            "  role: roles/cloudbuild.builds.builder\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
            "  role: roles/cloudbuild.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@containerregistry.iam.gserviceaccount.com\n",
            "  role: roles/containerregistry.ServiceAgent\n",
            "- members:\n",
            "  - user:bebechien@google.com\n",
            "  role: roles/owner\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-pubsub.iam.gserviceaccount.com\n",
            "  role: roles/pubsub.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:626103750140-compute@developer.gserviceaccount.com\n",
            "  role: roles/storage.admin\n",
            "- members:\n",
            "  - serviceAccount:626103750140-compute@developer.gserviceaccount.com\n",
            "  - serviceAccount:ray-test@ray-test-456802.iam.gserviceaccount.com\n",
            "  role: roles/storage.objectViewer\n",
            "etag: BwYytHolSdE=\n",
            "version: 1\n",
            "Updated IAM policy for project [ray-test-456802].\n",
            "bindings:\n",
            "- members:\n",
            "  - serviceAccount:ray-test@ray-test-456802.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.user\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
            "  role: roles/artifactregistry.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:626103750140-compute@developer.gserviceaccount.com\n",
            "  - serviceAccount:626103750140@cloudbuild.gserviceaccount.com\n",
            "  role: roles/cloudbuild.builds.builder\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
            "  role: roles/cloudbuild.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@containerregistry.iam.gserviceaccount.com\n",
            "  role: roles/containerregistry.ServiceAgent\n",
            "- members:\n",
            "  - user:bebechien@google.com\n",
            "  role: roles/owner\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-pubsub.iam.gserviceaccount.com\n",
            "  role: roles/pubsub.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:626103750140-compute@developer.gserviceaccount.com\n",
            "  role: roles/storage.admin\n",
            "- members:\n",
            "  - serviceAccount:626103750140-compute@developer.gserviceaccount.com\n",
            "  - serviceAccount:ray-test@ray-test-456802.iam.gserviceaccount.com\n",
            "  role: roles/storage.objectViewer\n",
            "etag: BwYytHpJ4mI=\n",
            "version: 1\n",
            "Updated IAM policy for project [ray-test-456802].\n",
            "bindings:\n",
            "- members:\n",
            "  - serviceAccount:626103750140-compute@developer.gserviceaccount.com\n",
            "  - serviceAccount:ray-test@ray-test-456802.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.user\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
            "  role: roles/artifactregistry.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:626103750140-compute@developer.gserviceaccount.com\n",
            "  - serviceAccount:626103750140@cloudbuild.gserviceaccount.com\n",
            "  role: roles/cloudbuild.builds.builder\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
            "  role: roles/cloudbuild.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@containerregistry.iam.gserviceaccount.com\n",
            "  role: roles/containerregistry.ServiceAgent\n",
            "- members:\n",
            "  - user:bebechien@google.com\n",
            "  role: roles/owner\n",
            "- members:\n",
            "  - serviceAccount:service-626103750140@gcp-sa-pubsub.iam.gserviceaccount.com\n",
            "  role: roles/pubsub.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:626103750140-compute@developer.gserviceaccount.com\n",
            "  role: roles/storage.admin\n",
            "- members:\n",
            "  - serviceAccount:626103750140-compute@developer.gserviceaccount.com\n",
            "  - serviceAccount:ray-test@ray-test-456802.iam.gserviceaccount.com\n",
            "  role: roles/storage.objectViewer\n",
            "etag: BwYytHptFww=\n",
            "version: 1\n"
          ]
        }
      ],
      "source": [
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "   --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
        "   --role=\"roles/storage.admin\"\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "   --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
        "   --role=\"roles/storage.objectViewer\"\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "   --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
        "   --role=\"roles/cloudbuild.builds.builder\"\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "   --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
        "   --role=\"roles/aiplatform.user\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek1-iTbPjzdJ"
      },
      "source": [
        "### Set tutorial folder\n",
        "\n",
        "Set up the folder to use in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbfKRabXj3la"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path as path\n",
        "\n",
        "root_path = path.cwd()\n",
        "tutorial_path = root_path / \"tutorial\"\n",
        "data_path = tutorial_path / \"data\"\n",
        "src_path = tutorial_path / \"src\"\n",
        "experiments_path = tutorial_path / \"experiments\"\n",
        "models_path = tutorial_path / \"models\"\n",
        "build_path = tutorial_path / \"build\"\n",
        "tests_path = tutorial_path / \"tests\"\n",
        "\n",
        "data_path.mkdir(parents=True, exist_ok=True)\n",
        "src_path.mkdir(parents=True, exist_ok=True)\n",
        "experiments_path.mkdir(parents=True, exist_ok=True)\n",
        "models_path.mkdir(parents=True, exist_ok=True)\n",
        "build_path.mkdir(parents=True, exist_ok=True)\n",
        "tests_path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ryiScCEapt"
      },
      "source": [
        "### Set a Ray cluster on Vertex AI\n",
        "\n",
        "Before running the code below, make sure to [set up](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/set-up) Ray on Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mlq_1NLEonF"
      },
      "outputs": [],
      "source": [
        "import vertex_ray\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from vertex_ray import NodeImages, Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dorIZFvjnGKL"
      },
      "source": [
        "#### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOOgvRJoQ6Xj"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15PYjIyOcx1d"
      },
      "source": [
        "#### Build the custom cluster image\n",
        "\n",
        " It's necessary to utilize Ray Custom cluster image support since certain dependencies are required.\n",
        "\n",
        " To use a custom cluster image, the first step is to build the image. Below there are the steps to cover:\n",
        "\n",
        "*  Prepare the requirements file\n",
        "*  Create the Dockerfile for the custom image\n",
        "*  Create the Docker image repository\n",
        "*  Build the Ray cluster custom image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5vW6XpnkFeR"
      },
      "source": [
        "##### Prepare the requirements file\n",
        "\n",
        "Prepare the `requirements` file that includes the dependencies your Ray application needs to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83KUQQylbrJR"
      },
      "outputs": [],
      "source": [
        "requirements = \"\"\"\n",
        "ipython==8.22.2\n",
        "torch==2.6.0\n",
        "ray==2.42.0\n",
        "ray[data]==2.42.0\n",
        "ray[train]==2.42.0\n",
        "ray[tune]==2.42.0\n",
        "datasets==3.5.0\n",
        "transformers==4.51.2\n",
        "evaluate==0.4.3\n",
        "rouge-score==0.1.2\n",
        "nltk==3.9.1\n",
        "accelerate==1.6.0\n",
        "bitsandbytes==0.45.5\n",
        "peft==0.15.1\n",
        "trl==0.16.1\n",
        "# flash-attn==2.5.5\n",
        "pyarrow==19.0.1\n",
        "fsspec==2024.12.0\n",
        "gcsfs==2024.12.0\n",
        "etils==1.12.2\n",
        "importlib-resources==6.5.2\n",
        "\"\"\"\n",
        "\n",
        "with open(build_path / \"requirements.txt\", \"w\") as rfile:\n",
        "    rfile.write(requirements)\n",
        "rfile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VHxHjCyKCyi"
      },
      "source": [
        "##### Create the Dockerfile\n",
        "\n",
        "Create the Dockerfile for the custom image by leveraging one of the prebuilt Ray on Vertex AI base images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu7bgcdeXZIS"
      },
      "outputs": [],
      "source": [
        "CUSTOM_BASE_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/ray-gpu.2-42.py311:latest\"  # @param [\"us-docker.pkg.dev/vertex-ai/training/ray-gpu.2-42.py311:latest\", \"europe-docker.pkg.dev/vertex-ai/training/ray-gpu.2-42.py311:latest\", \"asia-docker.pkg.dev/vertex-ai/training/ray-gpu.2-42.py311:latest\"] {allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRSzXRpxKB_Q"
      },
      "outputs": [],
      "source": [
        "dockerfile = f\"\"\"\n",
        "FROM {CUSTOM_BASE_IMAGE}\n",
        "\n",
        "# Install training libraries.\n",
        "ENV PIP_ROOT_USER_ACTION=ignore\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt\n",
        "\"\"\"\n",
        "\n",
        "with open(build_path / \"Dockerfile\", \"w\") as image_file:\n",
        "    image_file.write(dockerfile)\n",
        "image_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTU6Ai5_RT1f"
      },
      "source": [
        "##### Create the Docker image repository\n",
        "\n",
        "To store the custom cluster image, create a Docker repository in the Artifact Registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUoNxKGIRiGb"
      },
      "outputs": [],
      "source": [
        "REPO_NAME = f\"ray-test-{PROJECT_ID}-unique\"  # @param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh0iT2qVKBDi"
      },
      "outputs": [],
      "source": [
        "! gcloud artifacts repositories create {REPO_NAME} --repository-format=docker \\\n",
        "    --location={REGION} --description=\"Tutorial repository\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9lYOI15SFgh"
      },
      "source": [
        "##### Build the Ray cluster custom image\n",
        "\n",
        "Finally, build the Ray cluster custom image using Cloud Build."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR6BP7cgRLuD"
      },
      "outputs": [],
      "source": [
        "NODE_TRAIN_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/train\"\n",
        "BUILD_MACHINE_TYPE = \"E2_HIGHCPU_32\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VQKg4rMTLTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2ff56e-980e-46ee-f6f5-9a67fc2bf8f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating temporary archive of 2 file(s) totalling 549 bytes before compression.\n",
            "Uploading tarball of [/content/tutorial/build] to [gs://ray-test-456802_cloudbuild/source/1744799711.529789-4c29d670b65441feaab62ac361d43690.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/ray-test-456802/locations/us-central1/builds/09a6834d-1ec7-4082-86f0-e1935f907e71].\n",
            "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/09a6834d-1ec7-4082-86f0-e1935f907e71?project=626103750140 ].\n",
            "Waiting for build to complete. Polling interval: 1 second(s).\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"09a6834d-1ec7-4082-86f0-e1935f907e71\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://ray-test-456802_cloudbuild/source/1744799711.529789-4c29d670b65441feaab62ac361d43690.tgz#1744799712057594\n",
            "Copying gs://ray-test-456802_cloudbuild/source/1744799711.529789-4c29d670b65441feaab62ac361d43690.tgz#1744799712057594...\n",
            "/ [1 files][  584.0 B/  584.0 B]                                                \n",
            "Operation completed over 1 objects/584.0 B.\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon  3.072kB\n",
            "Step 1/4 : FROM us-docker.pkg.dev/vertex-ai/training/ray-gpu.2-42.py311:latest\n",
            "latest: Pulling from vertex-ai/training/ray-gpu.2-42.py311\n",
            "aece8493d397: Pulling fs layer\n",
            "5e3b7ee77381: Pulling fs layer\n",
            "5bd037f007fd: Pulling fs layer\n",
            "4cda774ad2ec: Pulling fs layer\n",
            "775f22adee62: Pulling fs layer\n",
            "263fc748118f: Pulling fs layer\n",
            "16c36d0187d0: Pulling fs layer\n",
            "e7a56570655c: Pulling fs layer\n",
            "507fc9045cba: Pulling fs layer\n",
            "23b7d8e07c16: Pulling fs layer\n",
            "922ac8fcb889: Pulling fs layer\n",
            "68075f2beca1: Pulling fs layer\n",
            "b3ff7e468ba0: Pulling fs layer\n",
            "cf6360694bc5: Pulling fs layer\n",
            "48eb49be2fd9: Pulling fs layer\n",
            "785d1d82f4f2: Pulling fs layer\n",
            "6dbe320b93d5: Pulling fs layer\n",
            "4cda774ad2ec: Waiting\n",
            "6ea407a5086d: Pulling fs layer\n",
            "775f22adee62: Waiting\n",
            "f483a0c677d4: Pulling fs layer\n",
            "263fc748118f: Waiting\n",
            "16c36d0187d0: Waiting\n",
            "e768f7172956: Pulling fs layer\n",
            "e7a56570655c: Waiting\n",
            "4f4fb700ef54: Pulling fs layer\n",
            "a4bd797779c1: Pulling fs layer\n",
            "fa147ce20db6: Pulling fs layer\n",
            "bb9dba1f9e33: Pulling fs layer\n",
            "ee690fec934d: Pulling fs layer\n",
            "58db82f4fa93: Pulling fs layer\n",
            "5bbd7afccfc2: Pulling fs layer\n",
            "cf6360694bc5: Waiting\n",
            "cfe511c8cee1: Pulling fs layer\n",
            "61aa2daaf777: Pulling fs layer\n",
            "3a840fdd9836: Pulling fs layer\n",
            "d5033dd60b2b: Pulling fs layer\n",
            "48eb49be2fd9: Waiting\n",
            "ff31d4a28830: Pulling fs layer\n",
            "f47413b32cf9: Pulling fs layer\n",
            "785d1d82f4f2: Waiting\n",
            "6dbe320b93d5: Waiting\n",
            "507fc9045cba: Waiting\n",
            "6ea407a5086d: Waiting\n",
            "23b7d8e07c16: Waiting\n",
            "922ac8fcb889: Waiting\n",
            "e768f7172956: Waiting\n",
            "f483a0c677d4: Waiting\n",
            "bb9dba1f9e33: Waiting\n",
            "ee690fec934d: Waiting\n",
            "58db82f4fa93: Waiting\n",
            "cfe511c8cee1: Waiting\n",
            "f47413b32cf9: Waiting\n",
            "fa147ce20db6: Waiting\n",
            "b3ff7e468ba0: Waiting\n",
            "a4bd797779c1: Waiting\n",
            "68075f2beca1: Waiting\n",
            "5bd037f007fd: Verifying Checksum\n",
            "5bd037f007fd: Download complete\n",
            "aece8493d397: Verifying Checksum\n",
            "aece8493d397: Download complete\n",
            "5e3b7ee77381: Verifying Checksum\n",
            "5e3b7ee77381: Download complete\n",
            "775f22adee62: Verifying Checksum\n",
            "775f22adee62: Download complete\n",
            "4cda774ad2ec: Verifying Checksum\n",
            "4cda774ad2ec: Download complete\n",
            "16c36d0187d0: Download complete\n",
            "e7a56570655c: Verifying Checksum\n",
            "e7a56570655c: Download complete\n",
            "507fc9045cba: Verifying Checksum\n",
            "507fc9045cba: Download complete\n",
            "922ac8fcb889: Verifying Checksum\n",
            "922ac8fcb889: Download complete\n",
            "aece8493d397: Pull complete\n",
            "5e3b7ee77381: Pull complete\n",
            "5bd037f007fd: Pull complete\n",
            "4cda774ad2ec: Pull complete\n",
            "775f22adee62: Pull complete\n",
            "263fc748118f: Verifying Checksum\n",
            "263fc748118f: Download complete\n",
            "b3ff7e468ba0: Verifying Checksum\n",
            "b3ff7e468ba0: Download complete\n",
            "cf6360694bc5: Verifying Checksum\n",
            "cf6360694bc5: Download complete\n",
            "68075f2beca1: Verifying Checksum\n",
            "68075f2beca1: Download complete\n",
            "48eb49be2fd9: Verifying Checksum\n",
            "48eb49be2fd9: Download complete\n",
            "785d1d82f4f2: Download complete\n",
            "6ea407a5086d: Verifying Checksum\n",
            "6ea407a5086d: Download complete\n",
            "6dbe320b93d5: Verifying Checksum\n",
            "6dbe320b93d5: Download complete\n",
            "e768f7172956: Verifying Checksum\n",
            "e768f7172956: Download complete\n",
            "a4bd797779c1: Verifying Checksum\n",
            "a4bd797779c1: Download complete\n",
            "f483a0c677d4: Verifying Checksum\n",
            "f483a0c677d4: Download complete\n",
            "fa147ce20db6: Download complete\n",
            "bb9dba1f9e33: Download complete\n",
            "58db82f4fa93: Verifying Checksum\n",
            "58db82f4fa93: Download complete\n",
            "5bbd7afccfc2: Verifying Checksum\n",
            "5bbd7afccfc2: Download complete\n",
            "cfe511c8cee1: Verifying Checksum\n",
            "cfe511c8cee1: Download complete\n",
            "61aa2daaf777: Download complete\n",
            "3a840fdd9836: Download complete\n",
            "ee690fec934d: Verifying Checksum\n",
            "ee690fec934d: Download complete\n",
            "ff31d4a28830: Verifying Checksum\n",
            "ff31d4a28830: Download complete\n",
            "d5033dd60b2b: Verifying Checksum\n",
            "d5033dd60b2b: Download complete\n",
            "23b7d8e07c16: Verifying Checksum\n",
            "23b7d8e07c16: Download complete\n",
            "f47413b32cf9: Download complete\n",
            "263fc748118f: Pull complete\n",
            "16c36d0187d0: Pull complete\n",
            "e7a56570655c: Pull complete\n",
            "507fc9045cba: Pull complete\n",
            "23b7d8e07c16: Pull complete\n",
            "922ac8fcb889: Pull complete\n",
            "68075f2beca1: Pull complete\n",
            "b3ff7e468ba0: Pull complete\n",
            "cf6360694bc5: Pull complete\n",
            "48eb49be2fd9: Pull complete\n",
            "785d1d82f4f2: Pull complete\n",
            "6dbe320b93d5: Pull complete\n",
            "6ea407a5086d: Pull complete\n",
            "f483a0c677d4: Pull complete\n",
            "e768f7172956: Pull complete\n",
            "4f4fb700ef54: Pull complete\n",
            "a4bd797779c1: Pull complete\n",
            "fa147ce20db6: Pull complete\n",
            "bb9dba1f9e33: Pull complete\n",
            "ee690fec934d: Pull complete\n",
            "58db82f4fa93: Pull complete\n",
            "5bbd7afccfc2: Pull complete\n",
            "cfe511c8cee1: Pull complete\n",
            "61aa2daaf777: Pull complete\n",
            "3a840fdd9836: Pull complete\n",
            "d5033dd60b2b: Pull complete\n",
            "ff31d4a28830: Pull complete\n",
            "f47413b32cf9: Pull complete\n",
            "Digest: sha256:efe653ee2f4ddf163e6a19fb0aee907a349be1db4de409a3e054d1f682dfdef7\n",
            "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/ray-gpu.2-42.py311:latest\n",
            " ---> 9e7f78a373ff\n",
            "Step 2/4 : ENV PIP_ROOT_USER_ACTION=ignore\n",
            " ---> Running in ead45821d4f8\n",
            "Removing intermediate container ead45821d4f8\n",
            " ---> bb93b4390d0a\n",
            "Step 3/4 : COPY requirements.txt .\n",
            " ---> 6630ead6fa40\n",
            "Step 4/4 : RUN pip install -r requirements.txt\n",
            " ---> Running in bd9e609f6fcb\n",
            "Collecting ipython==8.22.2 (from -r requirements.txt (line 2))\n",
            "  Downloading ipython-8.22.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting torch==2.6.0 (from -r requirements.txt (line 3))\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: ray==2.42.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (2.42.0)\n",
            "Collecting datasets==3.5.0 (from -r requirements.txt (line 8))\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting transformers==4.51.2 (from -r requirements.txt (line 9))\n",
            "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting evaluate==0.4.3 (from -r requirements.txt (line 10))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting rouge-score==0.1.2 (from -r requirements.txt (line 11))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting nltk==3.9.1 (from -r requirements.txt (line 12))\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting accelerate==1.6.0 (from -r requirements.txt (line 13))\n",
            "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting bitsandbytes==0.45.5 (from -r requirements.txt (line 14))\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting peft==0.15.1 (from -r requirements.txt (line 15))\n",
            "  Downloading peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl==0.16.1 (from -r requirements.txt (line 16))\n",
            "  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyarrow==19.0.1 (from -r requirements.txt (line 18))\n",
            "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting fsspec==2024.12.0 (from -r requirements.txt (line 19))\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting gcsfs==2024.12.0 (from -r requirements.txt (line 20))\n",
            "  Downloading gcsfs-2024.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting etils==1.12.2 (from -r requirements.txt (line 21))\n",
            "  Downloading etils-1.12.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting importlib-resources==6.5.2 (from -r requirements.txt (line 22))\n",
            "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython==8.22.2->-r requirements.txt (line 2)) (5.1.1)\n",
            "Collecting jedi>=0.16 (from ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting matplotlib-inline (from ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading prompt_toolkit-3.0.51-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython==8.22.2->-r requirements.txt (line 2)) (2.19.1)\n",
            "Collecting stack-data (from ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting traitlets>=5.13.0 (from ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pexpect>4.3 (from ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->-r requirements.txt (line 3)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->-r requirements.txt (line 3)) (4.12.2)\n",
            "Collecting networkx (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->-r requirements.txt (line 3)) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from ray==2.42.0->-r requirements.txt (line 4)) (8.1.8)\n",
            "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from ray==2.42.0->-r requirements.txt (line 4)) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from ray==2.42.0->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from ray==2.42.0->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.11/site-packages (from ray==2.42.0->-r requirements.txt (line 4)) (5.29.3)\n",
            "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from ray==2.42.0->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.11/site-packages (from ray==2.42.0->-r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.11/site-packages (from ray==2.42.0->-r requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from ray==2.42.0->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.0->-r requirements.txt (line 8)) (2.2.2)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.5.0->-r requirements.txt (line 8))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.0->-r requirements.txt (line 8)) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.0->-r requirements.txt (line 8)) (4.67.1)\n",
            "Collecting xxhash (from datasets==3.5.0->-r requirements.txt (line 8))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets==3.5.0->-r requirements.txt (line 8))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.0->-r requirements.txt (line 8)) (3.11.12)\n",
            "Collecting huggingface-hub>=0.24.0 (from datasets==3.5.0->-r requirements.txt (line 8))\n",
            "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.51.2->-r requirements.txt (line 9))\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.2->-r requirements.txt (line 9))\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers==4.51.2->-r requirements.txt (line 9))\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from rouge-score==0.1.2->-r requirements.txt (line 11)) (2.1.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge-score==0.1.2->-r requirements.txt (line 11)) (1.17.0)\n",
            "Collecting joblib (from nltk==3.9.1->-r requirements.txt (line 12))\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting psutil (from accelerate==1.6.0->-r requirements.txt (line 13))\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from trl==0.16.1->-r requirements.txt (line 16)) (13.9.4)\n",
            "Requirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.11/site-packages (from gcsfs==2024.12.0->-r requirements.txt (line 20)) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.11/site-packages (from gcsfs==2024.12.0->-r requirements.txt (line 20)) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.11/site-packages (from gcsfs==2024.12.0->-r requirements.txt (line 20)) (2.14.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.11/site-packages (from ray[train]==2.42.0->-r requirements.txt (line 6)) (2.6.2.2)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.6.0->-r requirements.txt (line 3))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 8)) (2.4.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 8)) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 8)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 8)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.5.0->-r requirements.txt (line 8)) (1.18.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.2->gcsfs==2024.12.0->-r requirements.txt (line 20)) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.2->gcsfs==2024.12.0->-r requirements.txt (line 20)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.2->gcsfs==2024.12.0->-r requirements.txt (line 20)) (4.9)\n",
            "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.5.0->-r requirements.txt (line 8)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.5.0->-r requirements.txt (line 8)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.5.0->-r requirements.txt (line 8)) (2025.1)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->ray==2.42.0->-r requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->ray==2.42.0->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->ray==2.42.0->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->ray==2.42.0->-r requirements.txt (line 4)) (2024.12.14)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from google-auth-oauthlib->gcsfs==2024.12.0->-r requirements.txt (line 20)) (2.0.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage->gcsfs==2024.12.0->-r requirements.txt (line 20)) (2.24.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage->gcsfs==2024.12.0->-r requirements.txt (line 20)) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage->gcsfs==2024.12.0->-r requirements.txt (line 20)) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage->gcsfs==2024.12.0->-r requirements.txt (line 20)) (1.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.6.0->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray==2.42.0->-r requirements.txt (line 4)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray==2.42.0->-r requirements.txt (line 4)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray==2.42.0->-r requirements.txt (line 4)) (0.22.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->trl==0.16.1->-r requirements.txt (line 16)) (3.0.0)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pure-eval (from stack-data->ipython==8.22.2->-r requirements.txt (line 2))\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs==2024.12.0->-r requirements.txt (line 20)) (1.67.0rc1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs==2024.12.0->-r requirements.txt (line 20)) (1.26.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl==0.16.1->-r requirements.txt (line 16)) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2024.12.0->-r requirements.txt (line 20)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2024.12.0->-r requirements.txt (line 20)) (3.2.2)\n",
            "Downloading ipython-8.22.2-py3-none-any.whl (811 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 812.0/812.0 kB 14.3 MB/s eta 0:00:00\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 766.7/766.7 MB 50.0 MB/s eta 0:00:00\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "Downloading transformers-4.51.2-py3-none-any.whl (10.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 80.3 MB/s eta 0:00:00\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 90.5 MB/s eta 0:00:00\n",
            "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.1/76.1 MB 86.3 MB/s eta 0:00:00\n",
            "Downloading peft-0.15.1-py3-none-any.whl (411 kB)\n",
            "Downloading trl-0.16.1-py3-none-any.whl (336 kB)\n",
            "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 MB 89.7 MB/s eta 0:00:00\n",
            "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Downloading gcsfs-2024.12.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading etils-1.12.2-py3-none-any.whl (167 kB)\n",
            "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 70.3 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 87.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 89.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 55.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 50.6 MB/s eta 0:00:00\n",
            "Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 84.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 89.8 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 63.1 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 62.4 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150.1/150.1 MB 84.6 MB/s eta 0:00:00\n",
            "Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.7/188.7 MB 84.7 MB/s eta 0:00:00\n",
            "Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 102.7 MB/s eta 0:00:00\n",
            "Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 86.5 MB/s eta 0:00:00\n",
            "Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 253.2/253.2 MB 83.8 MB/s eta 0:00:00\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 78.2 MB/s eta 0:00:00\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "Downloading prompt_toolkit-3.0.51-py3-none-any.whl (387 kB)\n",
            "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 792.7/792.7 kB 54.9 MB/s eta 0:00:00\n",
            "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 109.1 MB/s eta 0:00:00\n",
            "Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 84.0 MB/s eta 0:00:00\n",
            "Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 29.2 MB/s eta 0:00:00\n",
            "Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
            "Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py): started\n",
            "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=cd96589d9798110be556bc249bf1646e0ea684e5324554b8ca692650981cf717\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: wcwidth, triton, pure-eval, ptyprocess, nvidia-cusparselt-cu12, mpmath, xxhash, traitlets, sympy, safetensors, regex, pyarrow, psutil, prompt-toolkit, pexpect, parso, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, joblib, importlib-resources, fsspec, executing, etils, dill, asttokens, stack-data, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, matplotlib-inline, jedi, huggingface-hub, tokenizers, rouge-score, nvidia-cusolver-cu12, ipython, transformers, torch, datasets, evaluate, bitsandbytes, accelerate, trl, peft, gcsfs\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 19.0.0\n",
            "    Uninstalling pyarrow-19.0.0:\n",
            "      Successfully uninstalled pyarrow-19.0.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.2.0\n",
            "    Uninstalling fsspec-2025.2.0:\n",
            "      Successfully uninstalled fsspec-2025.2.0\n",
            "  Attempting uninstall: gcsfs\n",
            "    Found existing installation: gcsfs 2025.2.0\n",
            "    Uninstalling gcsfs-2025.2.0:\n",
            "      Successfully uninstalled gcsfs-2025.2.0\n",
            "Successfully installed accelerate-1.6.0 asttokens-3.0.0 bitsandbytes-0.45.5 datasets-3.5.0 dill-0.3.8 etils-1.12.2 evaluate-0.4.3 executing-2.2.0 fsspec-2024.12.0 gcsfs-2024.12.0 huggingface-hub-0.30.2 importlib-resources-6.5.2 ipython-8.22.2 jedi-0.19.2 joblib-1.4.2 matplotlib-inline-0.1.7 mpmath-1.3.0 multiprocess-0.70.16 networkx-3.4.2 nltk-3.9.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 parso-0.8.4 peft-0.15.1 pexpect-4.9.0 prompt-toolkit-3.0.51 psutil-7.0.0 ptyprocess-0.7.0 pure-eval-0.2.3 pyarrow-19.0.1 regex-2024.11.6 rouge-score-0.1.2 safetensors-0.5.3 stack-data-0.6.3 sympy-1.13.1 tokenizers-0.21.1 torch-2.6.0 traitlets-5.14.3 transformers-4.51.2 triton-3.2.0 trl-0.16.1 wcwidth-0.2.13 xxhash-3.5.0\n",
            "Removing intermediate container bd9e609f6fcb\n",
            " ---> aa468b250d29\n",
            "Successfully built aa468b250d29\n",
            "Successfully tagged us-central1-docker.pkg.dev/ray-test-456802/ray-test-ray-test-456802-unique/train:latest\n",
            "PUSH\n",
            "Pushing us-central1-docker.pkg.dev/ray-test-456802/ray-test-ray-test-456802-unique/train\n",
            "The push refers to repository [us-central1-docker.pkg.dev/ray-test-456802/ray-test-ray-test-456802-unique/train]\n",
            "fe5c729d51d3: Preparing\n",
            "9ac60624183b: Preparing\n",
            "ab20fffacdc5: Preparing\n",
            "cf4d7fae7358: Preparing\n",
            "ad3ddd532d53: Preparing\n",
            "1fcdadb33c2d: Preparing\n",
            "6988acc95019: Preparing\n",
            "ccf4052bd249: Preparing\n",
            "504fcbe3915d: Preparing\n",
            "0c635f108d4b: Preparing\n",
            "9b261b3cde44: Preparing\n",
            "869cd50a41ab: Preparing\n",
            "380dd498be16: Preparing\n",
            "380dd498be16: Preparing\n",
            "30251d4b58ab: Preparing\n",
            "5f70bf18a086: Preparing\n",
            "dab83da9ed34: Preparing\n",
            "608055287694: Preparing\n",
            "b41e573c8236: Preparing\n",
            "a2606a63e595: Preparing\n",
            "66e53eff4a6d: Preparing\n",
            "a56899f58112: Preparing\n",
            "0981690eb8dd: Preparing\n",
            "363a06a0bf01: Preparing\n",
            "383e6312d4f9: Preparing\n",
            "64758552f6fa: Preparing\n",
            "23d753990c8d: Preparing\n",
            "345cfa465206: Preparing\n",
            "dcb0f55f81ad: Preparing\n",
            "399d155a03b0: Preparing\n",
            "bc352a27a0e4: Preparing\n",
            "498bbcc60d01: Preparing\n",
            "6988acc95019: Waiting\n",
            "c0e21dcee623: Preparing\n",
            "d6b19a46b795: Preparing\n",
            "e6c05e83c163: Preparing\n",
            "256d88da4185: Preparing\n",
            "ccf4052bd249: Waiting\n",
            "504fcbe3915d: Waiting\n",
            "608055287694: Waiting\n",
            "0c635f108d4b: Waiting\n",
            "b41e573c8236: Waiting\n",
            "9b261b3cde44: Waiting\n",
            "5f70bf18a086: Waiting\n",
            "869cd50a41ab: Waiting\n",
            "dab83da9ed34: Waiting\n",
            "380dd498be16: Waiting\n",
            "a2606a63e595: Waiting\n",
            "66e53eff4a6d: Waiting\n",
            "30251d4b58ab: Waiting\n",
            "399d155a03b0: Waiting\n",
            "a56899f58112: Waiting\n",
            "bc352a27a0e4: Waiting\n",
            "0981690eb8dd: Waiting\n",
            "363a06a0bf01: Waiting\n",
            "498bbcc60d01: Waiting\n",
            "256d88da4185: Waiting\n",
            "c0e21dcee623: Waiting\n",
            "383e6312d4f9: Waiting\n",
            "64758552f6fa: Waiting\n",
            "23d753990c8d: Waiting\n",
            "345cfa465206: Waiting\n",
            "dcb0f55f81ad: Waiting\n",
            "d6b19a46b795: Waiting\n",
            "1fcdadb33c2d: Waiting\n",
            "ab20fffacdc5: Layer already exists\n",
            "cf4d7fae7358: Layer already exists\n",
            "ad3ddd532d53: Layer already exists\n",
            "1fcdadb33c2d: Layer already exists\n",
            "6988acc95019: Layer already exists\n",
            "ccf4052bd249: Layer already exists\n",
            "504fcbe3915d: Layer already exists\n",
            "0c635f108d4b: Layer already exists\n",
            "9b261b3cde44: Layer already exists\n",
            "869cd50a41ab: Layer already exists\n",
            "380dd498be16: Layer already exists\n",
            "30251d4b58ab: Layer already exists\n",
            "dab83da9ed34: Layer already exists\n",
            "5f70bf18a086: Layer already exists\n",
            "608055287694: Layer already exists\n",
            "9ac60624183b: Pushed\n",
            "b41e573c8236: Layer already exists\n",
            "a2606a63e595: Layer already exists\n",
            "66e53eff4a6d: Layer already exists\n",
            "a56899f58112: Layer already exists\n",
            "0981690eb8dd: Layer already exists\n",
            "363a06a0bf01: Layer already exists\n",
            "383e6312d4f9: Layer already exists\n",
            "23d753990c8d: Layer already exists\n",
            "64758552f6fa: Layer already exists\n",
            "345cfa465206: Layer already exists\n",
            "dcb0f55f81ad: Layer already exists\n",
            "399d155a03b0: Layer already exists\n",
            "bc352a27a0e4: Layer already exists\n",
            "498bbcc60d01: Layer already exists\n",
            "d6b19a46b795: Layer already exists\n",
            "c0e21dcee623: Layer already exists\n",
            "256d88da4185: Layer already exists\n",
            "e6c05e83c163: Layer already exists\n",
            "fe5c729d51d3: Pushed\n",
            "latest: digest: sha256:8f07d7940bedf24877cec8f40972305011d2d11ea9122a4d9085ffe7869f5969 size: 7865\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                         IMAGES                                                                                      STATUS\n",
            "09a6834d-1ec7-4082-86f0-e1935f907e71  2025-04-16T10:35:12+00:00  14M12S    gs://ray-test-456802_cloudbuild/source/1744799711.529789-4c29d670b65441feaab62ac361d43690.tgz  us-central1-docker.pkg.dev/ray-test-456802/ray-test-ray-test-456802-unique/train (+1 more)  SUCCESS\n"
          ]
        }
      ],
      "source": [
        "! gcloud builds submit --region={REGION} --tag={NODE_TRAIN_IMAGE} \\\n",
        "    --machine-type={BUILD_MACHINE_TYPE} --timeout=3600 {build_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O1xUMt7Z6r0"
      },
      "source": [
        "#### Create the Ray cluster\n",
        "\n",
        "With the custom image, create the Ray cluster using the custom image via Ray on Vertex AI SDK for Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZkHOH3v2i1p"
      },
      "outputs": [],
      "source": [
        "CLUSTER_NAME = f\"ray-test-{PROJECT_ID}-unique\"  # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd2TmEdyVLo1"
      },
      "source": [
        "###### Set the Ray cluster configuration\n",
        "\n",
        "Use the Vertex AI Python SDK for Ray on Vertex AI to set the cluster configuration.\n",
        "\n",
        "To know more about the cluster configuration, see the [documentation](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster#ray-on-vertex-ai-sdk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0BBynZrFn7z"
      },
      "outputs": [],
      "source": [
        "HEAD_NODE_MACHINE_TYPE = \"n1-standard-16\"  # @param {type:\"string\"}\n",
        "HEAD_NODE_COUNT = 1  # @param {type:\"integer\"}\n",
        "\n",
        "WORKER_NODE_MACHINE_TYPE = \"n1-standard-16\"  # @param {type:\"string\"}\n",
        "#WORKER_NODE_MACHINE_TYPE = \"a2-highgpu-1g\"  # @param {type:\"string\"}\n",
        "WORKER_NODE_COUNT = 1  # @param {type:\"integer\"}\n",
        "WORKER_ACCELERATION_TYPE = \"NVIDIA_TESLA_T4\"  # @param {type:\"string\"}\n",
        "#WORKER_ACCELERATION_TYPE = \"NVIDIA_TESLA_A100\"  # @param {type:\"string\"}\n",
        "WORKER_ACCELERATION_COUNT = 1  # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppNbbhgm3_GE"
      },
      "outputs": [],
      "source": [
        "HEAD_NODE_TYPE = Resources(\n",
        "    machine_type=HEAD_NODE_MACHINE_TYPE,\n",
        "    node_count=HEAD_NODE_COUNT,\n",
        ")\n",
        "\n",
        "WORKER_NODE_TYPES = [\n",
        "    Resources(\n",
        "        machine_type=WORKER_NODE_MACHINE_TYPE,\n",
        "        node_count=WORKER_NODE_COUNT,\n",
        "        accelerator_type=WORKER_ACCELERATION_TYPE,\n",
        "        accelerator_count=WORKER_ACCELERATION_COUNT,\n",
        "    )\n",
        "]\n",
        "\n",
        "CUSTOM_IMAGES = NodeImages(\n",
        "    head=NODE_TRAIN_IMAGE,\n",
        "    worker=NODE_TRAIN_IMAGE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfkpGqRjksaW"
      },
      "source": [
        "##### Create the Ray cluster\n",
        "\n",
        "Create the Ray cluster with the predefined custom configuration. Creating a cluster can take several minutes, depending on its configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-g6kLwqUj5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa72d589-b5eb-49b2-b0ea-259e4a91c21d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ray on Vertex AI]: Cluster State = 1\n",
            "Waiting for cluster provisioning; attempt 1; sleeping for 0:02:30 seconds\n",
            "[Ray on Vertex AI]: Cluster State = 1\n",
            "Waiting for cluster provisioning; attempt 2; sleeping for 0:01:54.750000 seconds\n",
            "[Ray on Vertex AI]: Cluster State = 1\n",
            "Waiting for cluster provisioning; attempt 3; sleeping for 0:01:27.783750 seconds\n",
            "[Ray on Vertex AI]: Cluster State = 1\n",
            "Waiting for cluster provisioning; attempt 4; sleeping for 0:01:07.154569 seconds\n",
            "[Ray on Vertex AI]: Cluster State = 1\n",
            "Waiting for cluster provisioning; attempt 5; sleeping for 0:00:51.373245 seconds\n",
            "[Ray on Vertex AI]: Cluster State = 1\n",
            "Waiting for cluster provisioning; attempt 6; sleeping for 0:00:39.300532 seconds\n",
            "[Ray on Vertex AI]: Cluster State = 1\n",
            "Waiting for cluster provisioning; attempt 7; sleeping for 0:00:30.064907 seconds\n",
            "[Ray on Vertex AI]: Cluster State = 1\n",
            "Waiting for cluster provisioning; attempt 8; sleeping for 0:00:30.064907 seconds\n",
            "[Ray on Vertex AI]: Cluster State = 3\n"
          ]
        }
      ],
      "source": [
        "ray_cluster_name = vertex_ray.create_ray_cluster(\n",
        "    head_node_type=HEAD_NODE_TYPE,\n",
        "    worker_node_types=WORKER_NODE_TYPES,\n",
        "    custom_images=CUSTOM_IMAGES,\n",
        "    cluster_name=CLUSTER_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmBlsbHAc2uO"
      },
      "source": [
        "##### Get the Ray cluster\n",
        "\n",
        "Use the Ray on Vertex AI SDK for Python to get the Ray cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UzG2WyXbZJi"
      },
      "outputs": [],
      "source": [
        "ray_clusters = vertex_ray.list_ray_clusters()\n",
        "ray_cluster_resource_name = ray_clusters[-1].cluster_resource_name\n",
        "ray_cluster = vertex_ray.get_ray_cluster(ray_cluster_resource_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7ZKdv5-GCWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f17cec71-8b89-48cd-a254-726074d22aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ray cluster on Vertex AI: projects/626103750140/locations/us-central1/persistentResources/ray-test-ray-test-456802-unique\n"
          ]
        }
      ],
      "source": [
        "print(\"Ray cluster on Vertex AI:\", ray_cluster_resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Import required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# General\n",
        "import io\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import string\n",
        "import time\n",
        "\n",
        "import datasets\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "# Ray - Training\n",
        "import ray\n",
        "import torch\n",
        "import transformers\n",
        "from etils import epath\n",
        "from google.cloud import storage\n",
        "from huggingface_hub import login\n",
        "from peft import PeftModel\n",
        "from ray.job_submission import JobStatus, JobSubmissionClient\n",
        "# Ray - Batch Serving\n",
        "from ray.tune import ExperimentAnalysis\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9_UttTcNGYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e08412-15a5-4200-bb29-e6316dbffcbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ray version:  2.42.0\n"
          ]
        }
      ],
      "source": [
        "print(\"Ray version: \", ray.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFgb-sZbBi8i"
      },
      "source": [
        "### Set variables\n",
        "\n",
        "Initiate some tutorial variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zykxFjqUB9jt"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata # `userdata` is a Colab API.\n",
        "\n",
        "# Training\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "EXPERIMENTS_FOLDER_URI = epath.Path(BUCKET_URI) / \"experiments\"\n",
        "TENSORBOARD_NAME = f\"rov-xsum-gemma-tb-{TIMESTAMP}\"\n",
        "\n",
        "# Serving\n",
        "MODELS_PATH = epath.Path(BUCKET_URI) / \"models\"\n",
        "PREDICTIONS_FOLDER_URI = epath.Path(BUCKET_URI) / \"predictions\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYQNboaOBk6B"
      },
      "source": [
        "### Define helpers\n",
        "\n",
        "Define an helper function to monitor the status of Ray job using Ray Dashboard API in your notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrvG6VDIm9hG"
      },
      "outputs": [],
      "source": [
        "def monitor_job(client, job_id):\n",
        "    \"\"\"Monitors the status of Ray job using Ray Dashboard API\"\"\"\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=f\"%(asctime)s.%(msecs)03d %(levelname)s {job_id} -- %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "        force=True,\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "        job_status = client.get_job_status(job_id)\n",
        "\n",
        "        if job_status == JobStatus.SUCCEEDED:\n",
        "            logging.info(\"Job succeeded!\")\n",
        "            break\n",
        "\n",
        "        elif job_status == JobStatus.FAILED:\n",
        "            logging.info(\"Job failed!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            logging.info(\"Job is running...\")\n",
        "            time.sleep(60)\n",
        "\n",
        "    return job_status\n",
        "\n",
        "\n",
        "def read_json_files(bucket_name, prefix=None):\n",
        "    \"\"\"Reads JSON files from a cloud storage bucket and returns a Pandas DataFrame\"\"\"\n",
        "\n",
        "    # Set up storage client\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=prefix)\n",
        "\n",
        "    dfs = []\n",
        "\n",
        "    for blob in blobs:\n",
        "        if blob.name.endswith(\".json\"):\n",
        "            file_bytes = blob.download_as_bytes()\n",
        "            file_string = file_bytes.decode(\"utf-8\")\n",
        "            with io.StringIO(file_string) as json_file:\n",
        "                df = pd.read_json(json_file, lines=True)\n",
        "            dfs.append(df)\n",
        "\n",
        "    return pd.concat(dfs, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkAwZBj71ipc"
      },
      "source": [
        "### Libraries settings\n",
        "\n",
        "Initiate some libraries settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3hT5TUM1lGc"
      },
      "outputs": [],
      "source": [
        "login(token=HF_TOKEN)\n",
        "datasets.disable_progress_bar()\n",
        "transformers.set_seed(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assh71yp4G_O"
      },
      "source": [
        "### Create a Vertex AI TensorBoard instance\n",
        "\n",
        "Create a Vertex AI TensorBoard instance for tracking and monitoring your tuning jobs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKyJ1vREZRIY"
      },
      "outputs": [],
      "source": [
        "tensorboard = vertex_ai.Tensorboard.create(\n",
        "    display_name=TENSORBOARD_NAME, project=PROJECT_ID, location=REGION\n",
        ")\n",
        "\n",
        "vertex_ai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        "    staging_bucket=BUCKET_URI,\n",
        "    experiment_tensorboard=tensorboard,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BH0TlP3PtTB"
      },
      "source": [
        "## Fine-Tune Gemma with Ray Train\n",
        "\n",
        "In this tutorial, you fine-tune Gemma 3 1B (`gemma-3-1b-it`) for summarizing newspaper articles using HuggingFace Transformer on Ray on Vertex AI. In an effort to make this notebook easily reproducible, you write a simple Python `trainer.py` script and submit it to the Ray cluster on Vertex AI using the Ray Jobs API via the public Ray Dashboard.\n",
        "\n",
        "As mentioned at the beginning, **consider this option for experimentation only.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du1IAdWos0AF"
      },
      "source": [
        "### Initialize the Ray package\n",
        "\n",
        "Create an `__init__.py` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWrX2WLUs4um"
      },
      "outputs": [],
      "source": [
        "with open(src_path / \"__init__.py\", \"a\") as init_file:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVL71KFDstVR"
      },
      "source": [
        "### Prepare the train script\n",
        "\n",
        "Create the `src/train.py` file which is the Python script for initializing Gemma fine-tuning using HuggingFace TRL library.\n",
        "\n",
        "See Also: https://docs.ray.io/en/latest/train/getting-started-transformers.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNQXWOHwtIDi"
      },
      "outputs": [],
      "source": [
        "train_script = '''\n",
        "# training libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import datasets\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import evaluate\n",
        "import ray\n",
        "import ray.train.huggingface.transformers\n",
        "\n",
        "def train_func(config):\n",
        "    # Helpers\n",
        "    def create_conversation(example):\n",
        "        return {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\",\n",
        "                    \"content\": f\"Summarize the following ARTICLE in one sentence.\\\\n###ARTICLE: {example['document']}\"},\n",
        "                {\"role\": \"assistant\",\n",
        "                    \"content\": f\"{example['summary']}\"}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def compute_metrics(eval_preds):\n",
        "        \"\"\"Helper function for computing metrics\"\"\"\n",
        "        preds, labels = eval_preds\n",
        "        preds = preds[0]\n",
        "\n",
        "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        metrics = rouge.compute(predictions=decoded_preds,\n",
        "                                references=decoded_labels,\n",
        "                                rouge_types=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n",
        "                                use_aggregator=True, use_stemmer=True)\n",
        "        metrics = {k: round(v * 100, 4) for k, v in metrics.items()}\n",
        "        return metrics\n",
        "\n",
        "    def preprocess_logits_for_metrics(logits, labels):\n",
        "        \"\"\"Helper function for logits preprocessing for metrics\"\"\"\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        return preds, labels\n",
        "\n",
        "    # Setting training\n",
        "    login(token=os.environ['HF_TOKEN'], add_to_git_credential=True)\n",
        "    transformers.set_seed(8)\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_id = \"xsum\"\n",
        "    dataset = datasets.load_dataset(dataset_id, trust_remote_code=True)\n",
        "    train_dataset = dataset[\"train\"].map(create_conversation, batched=False)\n",
        "    eval_dataset = dataset[\"test\"].map(create_conversation, batched=False)\n",
        "\n",
        "    # Preprocess dataset\n",
        "    model_id = \"google/gemma-3-1b-it\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.padding_side = 'right'\n",
        "\n",
        "    # Prepare model\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                                 quantization_config=bnb_config,\n",
        "                                                 device_map={'': torch.cuda.current_device()},\n",
        "                                                 torch_dtype=torch.bfloat16,\n",
        "                                                 # attn_implementation=\"flash_attention_2\"\n",
        "                                                 )\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=\"all-linear\",\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        modules_to_save=[\"lm_head\", \"embed_tokens\"] # make sure to save the lm_head and embed_tokens as you train the special tokens\n",
        "    )\n",
        "\n",
        "    # model.gradient_checkpointing_enable()\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "    training_args = SFTConfig(\n",
        "        output_dir=\"checkpoints\",\n",
        "        max_seq_length=512,\n",
        "        per_device_train_batch_size=config.get(\"per_device_train_batch_size\"),\n",
        "        per_device_eval_batch_size=config.get(\"per_device_eval_batch_size\"),\n",
        "        gradient_accumulation_steps=config.get(\"gradient_accumulation_steps\"),\n",
        "        logging_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_strategy=\"steps\",\n",
        "        max_steps=config.get(\"max_steps\"),\n",
        "        save_steps=config.get(\"save_steps\"),\n",
        "        logging_steps=config.get(\"logging_steps\"),\n",
        "        learning_rate=config.get(\"learning_rate\"),\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        bf16=False,\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "        ddp_find_unused_parameters=False,\n",
        "        gradient_checkpointing=True,\n",
        "        push_to_hub=False,\n",
        "        disable_tqdm=False,\n",
        "        load_best_model_at_end=False,\n",
        "        dataset_kwargs={\n",
        "            \"add_special_tokens\": False, # We template with special tokens\n",
        "            \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
        "        }\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        processing_class=tokenizer,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
        "        peft_config=lora_config,\n",
        "        #formatting_func=formatting_func\n",
        "    )\n",
        "    # model.config.use_cache = False\n",
        "\n",
        "    callback = ray.train.huggingface.transformers.RayTrainReportCallback()\n",
        "    trainer.add_callback(callback)\n",
        "    trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)\n",
        "    trainer.train()\n",
        "'''\n",
        "\n",
        "with open(src_path / \"train.py\", \"w\") as f:\n",
        "    f.write(train_script)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pznqJKCpEcL3"
      },
      "source": [
        "### Prepare the distributed training script\n",
        "\n",
        "Create `src/trainer.py` file which is the Python script for executing the Ray distributed training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCFfhM7RP4RI"
      },
      "outputs": [],
      "source": [
        "trainer_script = \"\"\"\n",
        "# libraries\n",
        "import argparse\n",
        "\n",
        "# training libraries\n",
        "from train import train_func\n",
        "\n",
        "# ray libraries\n",
        "import ray\n",
        "import ray.train.huggingface.transformers\n",
        "from ray.train import ScalingConfig, RunConfig, CheckpointConfig\n",
        "from ray.train.torch import TorchTrainer\n",
        "\n",
        "\n",
        "# helpers\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='Supervised tuning Gemma on Ray on Vertex AI')\n",
        "\n",
        "    # some gemma parameters\n",
        "    parser.add_argument(\"--train_batch_size\", type=int, default=1, help=\"train batch size\")\n",
        "    parser.add_argument(\"--eval_batch_size\", type=int, default=1, help=\"eval batch size\")\n",
        "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4, help=\"gradient accumulation steps\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4, help=\"learning rate\")\n",
        "    parser.add_argument(\"--max_steps\", type=int, default=100, help=\"max steps\")\n",
        "    parser.add_argument(\"--save_steps\", type=int, default=10, help=\"save steps\")\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=10, help=\"logging steps\")\n",
        "\n",
        "    # ray parameters\n",
        "    parser.add_argument('--num-workers', dest='num_workers', type=int, default=1, help='Number of workers')\n",
        "    parser.add_argument('--use-gpu', dest='use_gpu', action='store_true', default=False, help='Use GPU')\n",
        "    parser.add_argument('--experiment-name', dest='experiment_name', type=str, default='gemma-on-rov', help='Experiment name')\n",
        "    parser.add_argument('--logging-dir', dest='logging_dir', type=str, help='Logging directory')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    args = get_args()\n",
        "    config = vars(args)\n",
        "\n",
        "    # initialize ray session\n",
        "    ray.shutdown()\n",
        "    ray.init()\n",
        "\n",
        "    # training config\n",
        "    train_loop_config = {\n",
        "        \"per_device_train_batch_size\": config['train_batch_size'],\n",
        "        \"per_device_eval_batch_size\": config['eval_batch_size'],\n",
        "        \"gradient_accumulation_steps\": config['gradient_accumulation_steps'],\n",
        "        \"learning_rate\": config['learning_rate'],\n",
        "        \"max_steps\": config['max_steps'],\n",
        "        \"save_steps\": config['save_steps'],\n",
        "        \"logging_steps\": config['logging_steps'],\n",
        "    }\n",
        "    scaling_config = ScalingConfig(num_workers=config['num_workers'], use_gpu=config['use_gpu'])\n",
        "    run_config = RunConfig(checkpoint_config=CheckpointConfig(num_to_keep=10,\n",
        "                          checkpoint_score_attribute=\"loss\",\n",
        "                          checkpoint_score_order=\"min\"),\n",
        "                           storage_path=config['logging_dir'],\n",
        "                           name=config['experiment_name'])\n",
        "    trainer = TorchTrainer(\n",
        "        train_loop_per_worker=train_func,\n",
        "        train_loop_config=train_loop_config,\n",
        "        run_config=run_config,\n",
        "        scaling_config=scaling_config\n",
        "    )\n",
        "    # train\n",
        "    result = trainer.fit()\n",
        "\n",
        "    ray.shutdown()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open(src_path / \"trainer.py\", \"w\") as f:\n",
        "    f.write(trainer_script)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYcmfEvZ3C1i"
      },
      "source": [
        "### Submit a Ray job using the Ray Jobs API\n",
        "\n",
        "Submit the script to the Ray cluster on Vertex AI using the Ray Jobs API with  the public Ray dashboard address.\n",
        "\n",
        "Initiate the client to submit the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FljDHRQ63EP4"
      },
      "outputs": [],
      "source": [
        "client = JobSubmissionClient(\n",
        "    address=\"vertex_ray://{}\".format(ray_cluster.dashboard_address)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDHIlGlQJ2oi"
      },
      "source": [
        "Set some job configuration including experiment name, job id, training entrypoint and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxkqJ9vntz7C"
      },
      "outputs": [],
      "source": [
        "train_id = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=3))\n",
        "train_experiment_name = f\"rov-dialog-gemma-tune-{train_id}\"\n",
        "train_submission_id = f\"ray-job-{train_id}\"\n",
        "train_entrypoint = f\"python3 trainer.py --experiment-name={train_experiment_name} --logging-dir={EXPERIMENTS_FOLDER_URI} --num-workers={WORKER_NODE_COUNT} --use-gpu\"\n",
        "train_experiment_uri = EXPERIMENTS_FOLDER_URI / train_experiment_name\n",
        "train_runtime_env = {\n",
        "    \"working_dir\": str(src_path),\n",
        "    \"env_vars\": {\"HF_TOKEN\": HF_TOKEN, \"TORCH_NCCL_ASYNC_ERROR_HANDLING\": \"3\"},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhwkH-NzgEHS"
      },
      "source": [
        "Submit the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUPB6YKpur4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79dfa6d8-2586-4444-a8bd-ad8220679bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-16 11:35:27,028\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_d48cf906cb8c3c32.zip.\n",
            "2025-04-16 11:35:27,030\tINFO packaging.py:574 -- Creating a file package for local module '/content/tutorial/src'.\n"
          ]
        }
      ],
      "source": [
        "train_job_id = client.submit_job(\n",
        "    submission_id=train_submission_id,\n",
        "    entrypoint=train_entrypoint,\n",
        "    runtime_env=train_runtime_env,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T0MaYZegF_1"
      },
      "source": [
        "Check the status of the job while is running using the `monitor_job` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uCeasefuso8"
      },
      "outputs": [],
      "source": [
        "train_job_status = monitor_job(client, train_job_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67FI31SqKFdF"
      },
      "source": [
        "### Check training artifacts\n",
        "\n",
        "After the Ray training job has completed, see the model artifacts in the Cloud Storage location.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_JOYhXQKK8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c321d2e-d019-4bd8-f53d-369429eeb075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0  2025-04-16T11:35:44Z  gs://ray-test-ray-test-456802-unique/experiments/rov-dialog-gemma-tune-km5/\n",
            "         0  2025-04-16T11:35:45Z  gs://ray-test-ray-test-456802-unique/experiments/rov-dialog-gemma-tune-km5/.validate_storage_marker\n",
            "      7258  2025-04-16T14:25:54Z  gs://ray-test-ray-test-456802-unique/experiments/rov-dialog-gemma-tune-km5/basic-variant-state-2025-04-16_11-35-46.json\n",
            "     46062  2025-04-16T14:25:54Z  gs://ray-test-ray-test-456802-unique/experiments/rov-dialog-gemma-tune-km5/experiment_state-2025-04-16_11-35-46.json\n",
            "      1280  2025-04-16T11:35:45Z  gs://ray-test-ray-test-456802-unique/experiments/rov-dialog-gemma-tune-km5/trainer.pkl\n",
            "      1546  2025-04-16T11:35:45Z  gs://ray-test-ray-test-456802-unique/experiments/rov-dialog-gemma-tune-km5/tuner.pkl\n",
            "                                 gs://ray-test-ray-test-456802-unique/experiments/rov-dialog-gemma-tune-km5/TorchTrainer_eff0c_00000_0_2025-04-16_11-35-46/\n",
            "TOTAL: 6 objects, 56146 bytes (54.83 KiB)\n"
          ]
        }
      ],
      "source": [
        "! gsutil ls -l {train_experiment_uri}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAOAsmGYZ2l9"
      },
      "source": [
        "### Log metrics in Vertex AI TensorBoard\n",
        "\n",
        "Use Vertex AI TensorBoard for validating your training job by logging resulting metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><font color=red><b>NOTE</b></font></h1>\n",
        "\n",
        "To prevent the following error,\n",
        "\n",
        "```\n",
        "InvalidArgument: 400 User-specified resource ID 'rov-dialog-gemma-tune-km5-TorchTrainer-eff0c-00000-0-2025-04-16-11-35-46' must match the regular expression '[a-z0-9][a-z0-9-]{0,127}'\n",
        "```\n",
        "\n",
        "Rename your \"TorchTrainer\" folder using only lowercase letters.\n",
        "\n",
        "example:\n",
        "\n",
        "```\n",
        "! gsutil mv gs://ray-test-ray-test-456802-unique/experiments/rov-dialog-gemma-tune-km5/TorchTrainer_eff0c_00000_0_2025-04-16_11-35-46 gs://ray-test-ray-test-456802-unique/experiments/rov-dialog-gemma-tune-km5/trainer-0\n",
        "```\n",
        "\n",
        "After uploading your data to TensorBoard, remember to move the folder back to its original name to avoid future issues.\n"
      ],
      "metadata": {
        "id": "Cb0bsxsrH_hI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSMCKEp9Z8jx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639fae2c-0ddb-442a-912a-64ec4fd187cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboard:Please consider uploading to a new experiment instead of an existing one, as the former allows for better upload performance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View your Tensorboard at https://us-central1.tensorboard.googleusercontent.com/experiment/projects+ray-test-456802+locations+us-central1+tensorboards+7155265431798808576+experiments+rov-dialog-gemma-tune-km5\n",
            "\u001b[1m[2025-04-17T00:27:04]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2025-04-17T00:27:14]\u001b[0m Total uploaded: 160 scalars, 0 tensors, 0 binary objects\n"
          ]
        }
      ],
      "source": [
        "vertex_ai.upload_tb_log(\n",
        "    tensorboard_id=tensorboard.name,\n",
        "    tensorboard_experiment_name=train_experiment_name,\n",
        "    logdir=str(train_experiment_uri),\n",
        "    verbosity=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2RfyLpW0XO7"
      },
      "source": [
        "## Serving tuned Gemma model with Ray Data for offline predictions\n",
        "\n",
        "Using Ray on Vertex AI for developing AI/ML applications offers various benefits. In this scenario, you can use Cloud storage to conveniently store model checkpoints, metrics and more. This allows you to quickly consume the model for AI/ML downstreaming tasks including generating batch predictions using Ray Data.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "700NkAsLQHJI"
      },
      "source": [
        "### Generate predictions (locally)\n",
        "\n",
        "Generate predictions locally to validate the tuned model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN5YQ60PXbYq"
      },
      "source": [
        "#### Download Ray training checkpoints\n",
        "\n",
        "Download all resulting checkpoints from Ray job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBf0_agLhrLV"
      },
      "outputs": [],
      "source": [
        "! gsutil -q cp -r {train_experiment_uri}/* {experiments_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VavpxaY1hlSN"
      },
      "source": [
        "#### Get the best checkpoint\n",
        "\n",
        "Use the `ExperimentAnalysis` method to retrieve the the best checkpoint according to relevant metrics and mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUBEKpK1gVx9"
      },
      "outputs": [],
      "source": [
        "experiment_analysis = ExperimentAnalysis(experiments_path)\n",
        "log_path = experiment_analysis.get_best_trial(metric=\"eval_rougeLsum\", mode=\"max\")\n",
        "best_checkpoint = experiment_analysis.get_best_checkpoint(\n",
        "    log_path, metric=\"eval_rougeLsum\", mode=\"max\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icFuBCwJXSub"
      },
      "source": [
        "#### Load the model after training\n",
        "\n",
        "After training the model, load the model as described in the Hugging Face [documentation](https://huggingface.co/docs/trl/use_model#use-adapters-peft).\n",
        "\n",
        "Set the model and adapters path. Also set the path to store the resulting tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "370W-GHE7vM2"
      },
      "outputs": [],
      "source": [
        "base_model_path = \"google/gemma-3-1b-it\"\n",
        "peft_model_path = epath.Path(best_checkpoint.path) / \"checkpoint\"\n",
        "tuned_model_path = models_path / \"xsum-tuned-gemma-it\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UCc4hvGjES8"
      },
      "source": [
        "Initiate the associated Gemma tokenizer and base model. Also initiate the resulting adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbctXuXQAhLP"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path, device_map=\"auto\", torch_dtype=torch.float16\n",
        ")\n",
        "peft_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    peft_model_path,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    is_trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jytit8inMGy"
      },
      "source": [
        "Merge the base model and adapters to save the tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRGvb7hRnK-a"
      },
      "outputs": [],
      "source": [
        "tuned_model = peft_model.merge_and_unload()\n",
        "tuned_model.save_pretrained(tuned_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4WRLix49saQ"
      },
      "source": [
        "#### Generate summaries\n",
        "\n",
        "Generate summaries with the tuned model. Load the validation set of the tutorial dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrsR5HjNunzW"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_dataset(\n",
        "    \"xsum\", split=\"validation\", cache_dir=data_path, trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm5eFJNxoAHk"
      },
      "source": [
        "Sample one article to summarize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZjT8zAOPivt"
      },
      "outputs": [],
      "source": [
        "sample = dataset.select([random.randint(0, len(dataset) - 1)])\n",
        "document = sample[\"document\"][0]\n",
        "reference_summary = sample[\"summary\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16GiJy_EoEgi"
      },
      "source": [
        "Prepare the associated prompt following the [Gemma documentation](https://ai.google.dev/gemma/docs/formatting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFhA8R3wXPKj"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Summarize the following ARTICLE in one sentence.\\\\n###ARTICLE: {document}\",\n",
        "    },\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Lvss80objd"
      },
      "source": [
        "Initiate the text-generation pipeline for generating summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq6m6G-b_w7b"
      },
      "outputs": [],
      "source": [
        "tuned_gemma_pipeline = pipeline(\n",
        "    \"text-generation\", model=tuned_model, tokenizer=tokenizer, max_new_tokens=50\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb5GNgv1ovpc"
      },
      "source": [
        "Generate the associated summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuF2CMGx6AGi"
      },
      "outputs": [],
      "source": [
        "generated_tuned_gemma_summary = tuned_gemma_pipeline(\n",
        "    prompt, do_sample=True, temperature=0.1, add_special_tokens=True\n",
        ")[0][\"generated_text\"][len(prompt) :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKjU_Mhgo8yO"
      },
      "source": [
        "Print the generated summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq6HLy4Pu8oC"
      },
      "outputs": [],
      "source": [
        "print(f\"Reference summary: {reference_summary}\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"Tuned generated summary: {generated_tuned_gemma_summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdhPQC5aKmyP"
      },
      "source": [
        "#### Evaluate models\n",
        "\n",
        "As an additional step, you can evaluate the tuned model. To evaluate the model you compare models qualitatively and quantitatively.\n",
        "\n",
        "In one case, you compare responses generated by the base Gemma model with the ones generated by the tuned Gemma model. In the other case, you calculate ROUGE metrics and its improvements which gives you an idea of how well the tuned models is able to reproduce the reference summaries correctly with respect to the base model.\n",
        "\n",
        "Evaluate models by comparing generated summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmbCCJpWora7"
      },
      "outputs": [],
      "source": [
        "gemma_pipeline = pipeline(\n",
        "    \"text-generation\", model=base_model, tokenizer=tokenizer, max_new_tokens=50\n",
        ")\n",
        "\n",
        "generated_gemma_summary = gemma_pipeline(\n",
        "    prompt, do_sample=True, temperature=0.1, add_special_tokens=True\n",
        ")[0][\"generated_text\"][len(prompt) :]\n",
        "\n",
        "print(f\"Reference summary: {reference_summary}\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"Base generated summary: {generated_gemma_summary}\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"Tuned generated summary: {generated_tuned_gemma_summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLt-3ovZq7y1"
      },
      "source": [
        "Evaluate models by computing ROUGE metrics and its improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRSPxJVpfJ3E"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEO5qqtsMC75"
      },
      "outputs": [],
      "source": [
        "gemma_results = rouge.compute(\n",
        "    predictions=[generated_gemma_summary],\n",
        "    references=[reference_summary],\n",
        "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYC5t4I594pn"
      },
      "outputs": [],
      "source": [
        "tuned_gemma_results = rouge.compute(\n",
        "    predictions=[generated_tuned_gemma_summary],\n",
        "    references=[reference_summary],\n",
        "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnWVuRe394gh"
      },
      "outputs": [],
      "source": [
        "improvements = {}\n",
        "for rouge_metric, gemma_rouge in gemma_results.items():\n",
        "    tuned_gemma_rouge = tuned_gemma_results[rouge_metric]\n",
        "    if gemma_rouge != 0:\n",
        "        improvement = ((tuned_gemma_rouge - gemma_rouge) / gemma_rouge) * 100\n",
        "    else:\n",
        "        improvement = None\n",
        "    improvements[rouge_metric] = improvement\n",
        "\n",
        "print(\"Base Gemma vs Tuned Gemma - ROUGE improvements\")\n",
        "for rouge_metric, improvement in improvements.items():\n",
        "    print(f\"{rouge_metric}: {improvement:.3f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G51Xhb4H0fa1"
      },
      "source": [
        "### Batch prediction with Ray Data\n",
        "\n",
        "To generate batch prediction with the tuned model using Ray Data on Ray on Vertex AI, you need a dataset to generate predictions and the tuned model stored in the Cloud bucket.\n",
        "\n",
        "Then, you can leverage Ray Data which provides an easy-to-use API for offline batch inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjV1oHCTCe1r"
      },
      "source": [
        "#### Upload the tuned model\n",
        "\n",
        "Upload the tuned model on the Cloud storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFwD2p8jCjgH"
      },
      "outputs": [],
      "source": [
        "! gsutil -q cp -r {models_path} {MODELS_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIwOKSnrpxkW"
      },
      "source": [
        "#### Prepare the batch prediction training script\n",
        "\n",
        "Prepare `src/batch_predict.py` file which is the Python script for executing the Ray batch prediction job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekL5tV31pxkX"
      },
      "outputs": [],
      "source": [
        "batch_predictor_script = \"\"\"\n",
        "# General\n",
        "import argparse\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Serving\n",
        "import datasets\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.pipelines import pipeline\n",
        "\n",
        "# Ray\n",
        "import ray\n",
        "\n",
        "# Settings\n",
        "datasets.disable_progress_bar()\n",
        "\n",
        "# Variables\n",
        "base_model_path = \"google/gemma-3-1b-it\"\n",
        "\n",
        "\n",
        "# helpers\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='Batch prediction with Gemma on Ray on Vertex AI')\n",
        "    parser.add_argument('--tuned_model_path', type=str, help='path of adapter model')\n",
        "    parser.add_argument('--num_gpus', type=int, default=1, help='number of gpus')\n",
        "    parser.add_argument('--batch_size', type=int, default=8, help='batch size')\n",
        "    parser.add_argument('--sample_size', type=int, default=20, help='number of articles to summarize')\n",
        "    parser.add_argument('--temperature', type=float, default=0.1, help='temperature for generating summaries')\n",
        "    parser.add_argument('--max_new_tokens', type=int, default=50, help='max new token for generating summaries')\n",
        "    parser.add_argument('--output_dir', type=str, help='output directory for predictions')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Set configuration\n",
        "    args = get_args()\n",
        "    config = vars(args)\n",
        "\n",
        "    # Setting training\n",
        "    login(token=os.environ['HF_TOKEN'], add_to_git_credential=True)\n",
        "    transformers.set_seed(8)\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_id = \"xsum\"\n",
        "    sample_size = config[\"sample_size\"]\n",
        "    input_data = datasets.load_dataset(dataset_id, split=\"validation\", trust_remote_code=True)\n",
        "    input_data = input_data.select(range(sample_size))\n",
        "    ray_input_data = ray.data.from_huggingface(input_data)\n",
        "\n",
        "    # Generate predictions\n",
        "\n",
        "    class Summarizer:\n",
        "\n",
        "      def __init__(self):\n",
        "          self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "          self.tokenizer.padding_side = \"right\"\n",
        "\n",
        "          self.tuned_model = AutoModelForCausalLM.from_pretrained(config[\"tuned_model_path\"],\n",
        "                                                                  device_map='auto',\n",
        "                                                                  torch_dtype=torch.float16)\n",
        "\n",
        "          self.pipeline = pipeline(\"text-generation\",\n",
        "                                    model=self.tuned_model,\n",
        "                                    tokenizer=self.tokenizer,\n",
        "                                    max_new_tokens=config[\"max_new_tokens\"])\n",
        "\n",
        "      def __call__(self, batch: np.ndarray):\n",
        "\n",
        "          # prepare dataset\n",
        "          messages = [{\"role\": \"user\",\n",
        "                      \"content\": f\"Summarize the following ARTICLE in one sentence.\\\\n###ARTICLE: {document}\"}\n",
        "                      for document in batch[\"document\"]]\n",
        "\n",
        "          batch['prompt'] = [self.tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
        "                             for message in messages]\n",
        "\n",
        "          # generate\n",
        "          batch['generated_summary'] = [self.pipeline(prompt,\n",
        "                                                    do_sample=True,\n",
        "                                                    temperature=config[\"temperature\"],\n",
        "                                                    add_special_tokens=True)[0][\"generated_text\"][len(prompt):]\n",
        "                                                    for prompt in batch['prompt']]\n",
        "\n",
        "          return batch\n",
        "\n",
        "\n",
        "    predictions_data = ray_input_data.map_batches(\n",
        "        Summarizer,\n",
        "        concurrency=config[\"num_gpus\"],\n",
        "        num_gpus=1,\n",
        "        batch_size=config['batch_size'])\n",
        "\n",
        "    # Store resulting predictions\n",
        "    predictions_data.write_json(config[\"output_dir\"], try_create_dir=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open(src_path / \"batch_predictor.py\", \"w\") as f:\n",
        "    f.write(batch_predictor_script)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xynt8impxkX"
      },
      "source": [
        "####  Submit a Ray job using the Ray Jobs API\n",
        "\n",
        "Submit the script to the Ray on Vertex AI cluster using the Ray Jobs API via  the public Ray dashboard address.\n",
        "\n",
        "Initiate the client to submit the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hY96qSQFpxkX"
      },
      "outputs": [],
      "source": [
        "client = JobSubmissionClient(\n",
        "    address=\"vertex_ray://{}\".format(ray_cluster.dashboard_address)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ6EqvIF5q9F"
      },
      "source": [
        "Set some job configuration including model path, job id, prediction entrypoint and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDycAWy45siE"
      },
      "outputs": [],
      "source": [
        "batch_predict_id = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
        "batch_predict_submission_id = f\"ray-job-{batch_predict_id}\"\n",
        "tuned_model_uri_path = str(MODELS_PATH / \"xsum-tuned-gemma-it\").replace(\n",
        "    \"gs://\", \"/gcs/\"\n",
        ")\n",
        "batch_predict_entrypoint = f\"python3 batch_predictor.py --tuned_model_path={tuned_model_uri_path} --num_gpus=2 --output_dir={PREDICTIONS_FOLDER_URI}\"\n",
        "batch_predict_runtime_env = {\n",
        "    \"working_dir\": str(src_path),\n",
        "    \"env_vars\": {\"HF_TOKEN\": HF_TOKEN},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcaMEKz6pxkX"
      },
      "source": [
        "Submit the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qADWE5LOpxkX"
      },
      "outputs": [],
      "source": [
        "batch_predict_job_id = client.submit_job(\n",
        "    submission_id=batch_predict_submission_id,\n",
        "    entrypoint=batch_predict_entrypoint,\n",
        "    runtime_env=batch_predict_runtime_env,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxrOwuVf6f3R"
      },
      "source": [
        "Check the status of the job using the `monitor_job` helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXTF8pp9KJS8"
      },
      "outputs": [],
      "source": [
        "batch_predict_job_status = monitor_job(client, batch_predict_job_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2Iafitj7wtH"
      },
      "source": [
        "#### Get generated summaries\n",
        "\n",
        "Have a quick view of generated summaries using a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em4WQokY7050"
      },
      "outputs": [],
      "source": [
        "predictions_df = read_json_files(prefix=\"predictions/\", bucket_name=BUCKET_NAME)\n",
        "predictions_df = predictions_df[\n",
        "    [\"id\", \"document\", \"prompt\", \"summary\", \"generated_summary\"]\n",
        "]\n",
        "predictions_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) that you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources that you created in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "delete_tensorboards = False\n",
        "delete_experiments = False\n",
        "delete_ray_clusters = False\n",
        "delete_image_repo = False\n",
        "delete_bucket = False\n",
        "delete_tutorial = False\n",
        "\n",
        "# Delete tensorboard\n",
        "if delete_tensorboards:\n",
        "    tensorboard_list = vertex_ai.Tensorboard.list()\n",
        "    for tensorboard in tensorboard_list:\n",
        "        tensorboard.delete()\n",
        "\n",
        "# Delete experiments\n",
        "if delete_experiments:\n",
        "    experiment_list = vertex_ai.Experiment.list()\n",
        "    for experiment in experiment_list:\n",
        "        experiment.delete()\n",
        "\n",
        "# Delete ray on vertex cluster\n",
        "if delete_ray_clusters:\n",
        "    ray_cluster_list = vertex_ray.list_ray_clusters()\n",
        "    for ray_cluster in ray_cluster_list:\n",
        "        vertex_ray.delete_ray_cluster(ray_cluster.cluster_resource_name)\n",
        "\n",
        "if delete_image_repo:\n",
        "    ! gcloud artifacts repositories delete {REPO_NAME}\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "if delete_bucket:\n",
        "    ! gsutil -q -m rm -r {BUCKET_URI}\n",
        "\n",
        "# Delete tutorial folder\n",
        "if delete_tutorial:\n",
        "    shutil.rmtree(tutorial_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}